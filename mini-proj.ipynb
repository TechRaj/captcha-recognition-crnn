{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T09:40:25.157072Z",
     "iopub.status.busy": "2025-11-08T09:40:25.156859Z",
     "iopub.status.idle": "2025-11-08T09:40:25.379649Z",
     "shell.execute_reply": "2025-11-08T09:40:25.379042Z",
     "shell.execute_reply.started": "2025-11-08T09:40:25.157056Z"
    },
    "id": "XioAgIn5QTIj",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def resize_and_pad(img, target_size=(40, 120)):\n",
    "    \"\"\"\n",
    "    Resize image proportionally and pad to target size.\n",
    "    \n",
    "    Args:\n",
    "        img: Input image\n",
    "        target_size: Target (height, width) tuple\n",
    "    \n",
    "    Returns:\n",
    "        Padded image of target size\n",
    "    \"\"\"\n",
    "    target_h, target_w = target_size\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # Resize proportionally to fit within target dimensions\n",
    "    scale = min(target_w / w, target_h / h)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Create canvas\n",
    "    padded = np.ones((target_h, target_w), dtype=img.dtype) * 255\n",
    "\n",
    "    # Center the resized image\n",
    "    x_offset = (target_w - new_w) // 2\n",
    "    y_offset = (target_h - new_h) // 2\n",
    "    padded[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized\n",
    "\n",
    "    return padded\n",
    "\n",
    "\n",
    "def remove_black_lines(img):\n",
    "    \"\"\"\n",
    "    Remove black strikethrough lines from CAPTCHA images using inpainting.\n",
    "    \n",
    "    Args:\n",
    "        img: Input BGR image\n",
    "    \n",
    "    Returns:\n",
    "        cleaned: Image with black lines removed\n",
    "        mask_black: Binary mask of removed regions\n",
    "    \"\"\"\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define HSV range for dark colors (black and dark gray)\n",
    "    lower_black = np.array([0, 0, 0])\n",
    "    upper_black = np.array([180, 255, 80])\n",
    "\n",
    "    # Create mask for black regions\n",
    "    mask_black = cv2.inRange(hsv, lower_black, upper_black)\n",
    "\n",
    "    # Inpaint to repair text where black lines were removed\n",
    "    cleaned = cv2.inpaint(img, mask_black, inpaintRadius=1, flags=cv2.INPAINT_TELEA)\n",
    "\n",
    "    return cleaned, mask_black\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T09:40:31.384664Z",
     "iopub.status.busy": "2025-11-08T09:40:31.384400Z",
     "iopub.status.idle": "2025-11-08T09:40:35.096764Z",
     "shell.execute_reply": "2025-11-08T09:40:35.096117Z",
     "shell.execute_reply.started": "2025-11-08T09:40:31.384645Z"
    },
    "id": "rHBnnI5T6qP4",
    "outputId": "1d75c60d-ca88-43ce-de31-b32d137b0e96",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "class CaptchaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Basic CAPTCHA dataset for initial preprocessing exploration.\n",
    "    Loads images, removes black lines, converts to grayscale, and equalizes histogram.\n",
    "    \"\"\"\n",
    "    def __init__(self, folder):\n",
    "        self.folder = folder\n",
    "        self.files = os.listdir(folder)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.files[idx]\n",
    "        img_path = os.path.join(self.folder, filename)\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        # Remove black strikethrough lines\n",
    "        img, _ = remove_black_lines(img)\n",
    "\n",
    "        # Convert to grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Enhance contrast using histogram equalization\n",
    "        img = cv2.equalizeHist(img)\n",
    "\n",
    "        # Resize and pad to fixed dimensions\n",
    "        img = resize_and_pad(img, target_size=(80, 280))\n",
    "\n",
    "        # Extract label from filename (format: label-0.png)\n",
    "        label = filename.split('-')[0]\n",
    "\n",
    "        return img, label, filename\n",
    "\n",
    "# Load dataset for visualization\n",
    "dataset = CaptchaDataset(\"/kaggle/input/captcha-training-images/captacha_dataset/train\")\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Display sample image\n",
    "img = cv2.imread(\"/kaggle/input/captcha-training-images/captacha_dataset/train/0024miih-0.png\")\n",
    "print(img.shape)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T09:41:07.273312Z",
     "iopub.status.busy": "2025-11-08T09:41:07.273022Z",
     "iopub.status.idle": "2025-11-08T09:41:07.672950Z",
     "shell.execute_reply": "2025-11-08T09:41:07.672377Z",
     "shell.execute_reply.started": "2025-11-08T09:41:07.273288Z"
    },
    "id": "jUiuRduG6vlc",
    "outputId": "83a3bb68-4327-4cd3-ac5c-a05686bbcbee",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualize preprocessing pipeline: original vs transformed images\n",
    "images, labels, filenames = next(iter(loader))\n",
    "\n",
    "plt.figure(figsize=(15, 2))\n",
    "for i in range(len(images)):\n",
    "    label = labels[i]\n",
    "    filename = filenames[i]\n",
    "\n",
    "    # Load original image\n",
    "    ori_img = cv2.imread(f\"/kaggle/input/captcha-training-images/captacha_dataset/train/{filename}\")\n",
    "    plt.subplot(1, 2, 1)\n",
    "    if ori_img is not None:\n",
    "        plt.imshow(ori_img, cmap=\"gray\")\n",
    "        plt.title(f\"Original: {label}\")\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, f\"Failed to load:\\n{filename}\", ha='center', va='center')\n",
    "        plt.title(\"Load Error\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Display preprocessed image\n",
    "    transformed_img = images[i].numpy()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(transformed_img, cmap=\"gray\")\n",
    "    plt.title(f\"Preprocessed: {label}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T09:41:11.023126Z",
     "iopub.status.busy": "2025-11-08T09:41:11.022636Z",
     "iopub.status.idle": "2025-11-08T09:41:11.027954Z",
     "shell.execute_reply": "2025-11-08T09:41:11.027397Z",
     "shell.execute_reply.started": "2025-11-08T09:41:11.023102Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define character vocabulary for CAPTCHAs\n",
    "CHARS = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "# Create character-to-index and index-to-character mappings\n",
    "# Index 0 is reserved for CTC blank token\n",
    "char_to_idx = {c: i + 1 for i, c in enumerate(CHARS)}\n",
    "idx_to_char = {i + 1: c for i, c in enumerate(CHARS)}\n",
    "idx_to_char[0] = \"\"  # CTC blank token\n",
    "\n",
    "num_classes = len(CHARS) + 1\n",
    "print(\"Total classes (incl. blank):\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T09:41:13.861507Z",
     "iopub.status.busy": "2025-11-08T09:41:13.860793Z",
     "iopub.status.idle": "2025-11-08T09:41:13.865785Z",
     "shell.execute_reply": "2025-11-08T09:41:13.865038Z",
     "shell.execute_reply.started": "2025-11-08T09:41:13.861479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def encode_label(text):\n",
    "    \"\"\"\n",
    "    Convert text string to list of token indices.\n",
    "    \n",
    "    Args:\n",
    "        text: String to encode\n",
    "    \n",
    "    Returns:\n",
    "        List of integer token IDs\n",
    "    \"\"\"\n",
    "    return [char_to_idx[c] for c in text if c in char_to_idx]\n",
    "\n",
    "def decode_label(tokens):\n",
    "    \"\"\"\n",
    "    Convert token indices back to string.\n",
    "    CTC will handle collapsing repeats and removing blanks during decoding.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of integer token IDs\n",
    "    \n",
    "    Returns:\n",
    "        Decoded string\n",
    "    \"\"\"\n",
    "    return \"\".join(idx_to_char.get(t, \"\") for t in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T09:41:15.960156Z",
     "iopub.status.busy": "2025-11-08T09:41:15.959885Z",
     "iopub.status.idle": "2025-11-08T09:41:20.971915Z",
     "shell.execute_reply": "2025-11-08T09:41:20.971326Z",
     "shell.execute_reply.started": "2025-11-08T09:41:15.960135Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# PyTorch Dataset for CTC Training with Data Augmentation\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "class AugmentTransform:\n",
    "    \"\"\"\n",
    "    Data augmentation pipeline for CAPTCHA images.\n",
    "    Applies transformations to match real CAPTCHA distortions and improve model robustness.\n",
    "    \n",
    "    Transformations include:\n",
    "    - Random rotation\n",
    "    - Random shear/skew\n",
    "    - Random translation\n",
    "    - Random black diagonal lines (noise)\n",
    "    - Random brightness adjustment\n",
    "    - Random Gaussian noise\n",
    "    \"\"\"\n",
    "    def __init__(self, training=True):\n",
    "        self.training = training\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Apply augmentation pipeline to grayscale numpy image.\n",
    "        \n",
    "        Args:\n",
    "            img: Grayscale numpy array\n",
    "        \n",
    "        Returns:\n",
    "            Augmented image\n",
    "        \"\"\"\n",
    "        if not self.training:\n",
    "            return img\n",
    "        \n",
    "        # Convert to PIL for geometric transforms\n",
    "        img_pil = Image.fromarray(img)\n",
    "        \n",
    "        # Random rotation (±5 degrees)\n",
    "        if random.random() > 0.5:\n",
    "            angle = random.uniform(-5, 5)\n",
    "            img_pil = TF.rotate(img_pil, angle, fill=255)\n",
    "        \n",
    "        # Random shear/skew to match CAPTCHA slanted text\n",
    "        if random.random() > 0.5:\n",
    "            shear_x = random.uniform(-10, 10)\n",
    "            img_pil = TF.affine(img_pil, angle=0, translate=(0, 0), \n",
    "                                scale=1.0, shear=shear_x, fill=255)\n",
    "        \n",
    "        # Random translation\n",
    "        if random.random() > 0.5:\n",
    "            translate = (random.randint(-3, 3), random.randint(-2, 2))\n",
    "            img_pil = TF.affine(img_pil, angle=0, translate=translate, \n",
    "                                scale=1.0, shear=0, fill=255)\n",
    "        \n",
    "        # Convert back to numpy\n",
    "        img = np.array(img_pil)\n",
    "        \n",
    "        # Add random black diagonal lines to simulate CAPTCHA noise (70% chance)\n",
    "        if random.random() > 0.3:\n",
    "            num_lines = random.randint(1, 2)\n",
    "            for _ in range(num_lines):\n",
    "                x1 = random.randint(0, img.shape[1])\n",
    "                y1 = random.randint(0, img.shape[0])\n",
    "                x2 = random.randint(0, img.shape[1])\n",
    "                y2 = random.randint(0, img.shape[0])\n",
    "                thickness = random.randint(1, 2)\n",
    "                cv2.line(img, (x1, y1), (x2, y2), color=0, thickness=thickness)\n",
    "        \n",
    "        # Random brightness adjustment\n",
    "        if random.random() > 0.5:\n",
    "            factor = random.uniform(0.85, 1.15)\n",
    "            img = np.clip(img * factor, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Random Gaussian noise\n",
    "        if random.random() > 0.5:\n",
    "            noise = np.random.normal(0, 2, img.shape)\n",
    "            img = np.clip(img + noise, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        return img\n",
    "\n",
    "class CaptchaCTCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for CAPTCHA images with CTC encoding.\n",
    "    Handles preprocessing, augmentation, and label encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, folder, augment=False, clean_files=None):\n",
    "        \"\"\"\n",
    "        Initialize CAPTCHA dataset.\n",
    "        \n",
    "        Args:\n",
    "            folder: Path to image folder\n",
    "            augment: Whether to apply data augmentation\n",
    "            clean_files: Optional list of filenames to use (for filtered training)\n",
    "        \"\"\"\n",
    "        self.folder = folder\n",
    "        \n",
    "        # Use clean_files if provided, otherwise use all files\n",
    "        if clean_files is not None:\n",
    "            self.files = clean_files\n",
    "            print(f\"Using {len(self.files)} clean/filtered samples\")\n",
    "        else:\n",
    "            self.files = [f for f in os.listdir(folder) if f.endswith('.png')]\n",
    "            print(f\"Using all {len(self.files)} samples\")\n",
    "        \n",
    "        self.augment = AugmentTransform(training=augment)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and preprocess a single CAPTCHA image.\n",
    "        \n",
    "        Returns:\n",
    "            img: Normalized tensor of shape (1, H, W)\n",
    "            label_encoded: List of character indices\n",
    "            label: Original text label\n",
    "        \"\"\"\n",
    "        filename = self.files[idx]\n",
    "        img_path = os.path.join(self.folder, filename)\n",
    "\n",
    "        # Load image and apply preprocessing\n",
    "        img = cv2.imread(img_path)\n",
    "        img, _ = remove_black_lines(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply augmentation if enabled\n",
    "        img = self.augment(img)\n",
    "        \n",
    "        # Resize and pad to fixed dimensions\n",
    "        img = resize_and_pad(img, target_size=(80, 280))\n",
    "        \n",
    "        # Normalize to [0, 1] and convert to tensor\n",
    "        img = img.astype('float32') / 255.0\n",
    "        img = torch.tensor(img).unsqueeze(0)  # (1, H, W) - add channel dimension\n",
    "        \n",
    "        # Extract label from filename (format: label-0.png)\n",
    "        label = filename.split('-')[0]\n",
    "        \n",
    "        # Encode label to character indices\n",
    "        label_encoded = encode_label(label)\n",
    "        \n",
    "        return img, label_encoded, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T09:41:23.867168Z",
     "iopub.status.busy": "2025-11-08T09:41:23.866379Z",
     "iopub.status.idle": "2025-11-08T09:41:23.912246Z",
     "shell.execute_reply": "2025-11-08T09:41:23.911697Z",
     "shell.execute_reply.started": "2025-11-08T09:41:23.867140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Custom collate function for CTC - handles variable length labels\n",
    "def ctc_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader to handle variable-length labels for CTC.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of tuples (image, label_encoded, label_text)\n",
    "    \n",
    "    Returns:\n",
    "        images: Tensor of shape (batch_size, 1, H, W)\n",
    "        targets: Concatenated label indices\n",
    "        input_lengths: Length of each sequence from CNN output\n",
    "        target_lengths: Length of each label\n",
    "        label_texts: Original text labels (for debugging)\n",
    "    \"\"\"\n",
    "    images, labels, label_texts = zip(*batch)\n",
    "    \n",
    "    # Stack images into a batch\n",
    "    images = torch.stack(images, dim=0)  # (batch_size, 1, H, W)\n",
    "    \n",
    "    # Concatenate all labels into a single list (required by PyTorch CTC)\n",
    "    targets = []\n",
    "    target_lengths = []\n",
    "    for label in labels:\n",
    "        targets.extend(label)\n",
    "        target_lengths.append(len(label))\n",
    "    \n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "    target_lengths = torch.tensor(target_lengths, dtype=torch.long)\n",
    "    \n",
    "    # Input lengths - depends on CNN architecture\n",
    "    # For width 280 with 2 max pools (stride 2): 280 / 4 = 70 time steps\n",
    "    batch_size = images.size(0)\n",
    "    input_lengths = torch.full((batch_size,), 70, dtype=torch.long)  # Adjust based on CNN\n",
    "    \n",
    "    return images, targets, input_lengths, target_lengths, label_texts\n",
    "\n",
    "# Create PyTorch datasets with data augmentation for training\n",
    "train_dataset = CaptchaCTCDataset(\"/kaggle/input/captcha-training-images/captacha_dataset/train\", augment=True)\n",
    "test_dataset = CaptchaCTCDataset(\"/kaggle/input/captcha-training-images/captacha_dataset/test\", augment=False)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T09:41:27.957706Z",
     "iopub.status.busy": "2025-11-08T09:41:27.957094Z",
     "iopub.status.idle": "2025-11-08T09:41:27.962852Z",
     "shell.execute_reply": "2025-11-08T09:41:27.962054Z",
     "shell.execute_reply.started": "2025-11-08T09:41:27.957681Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create PyTorch DataLoaders with custom collate function\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=ctc_collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=ctc_collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T09:41:30.479892Z",
     "iopub.status.busy": "2025-11-08T09:41:30.479231Z",
     "iopub.status.idle": "2025-11-08T09:42:22.800656Z",
     "shell.execute_reply": "2025-11-08T09:42:22.799799Z",
     "shell.execute_reply.started": "2025-11-08T09:41:30.479865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Validate dataset statistics\n",
    "print(\"=== Dataset Statistics ===\")\n",
    "print(f\"Total training samples: {len(train_dataset)}\")\n",
    "print(f\"Total test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Extract all labels from dataset\n",
    "train_labels = [train_dataset[i][2] for i in range(len(train_dataset))]  # Get label text\n",
    "test_labels = [test_dataset[i][2] for i in range(len(test_dataset))]\n",
    "\n",
    "# Check label lengths\n",
    "train_lengths = [len(label) for label in train_labels]\n",
    "print(f\"\\nLabel length statistics:\")\n",
    "print(f\"  Min label length: {min(train_lengths)}\")\n",
    "print(f\"  Max label length: {max(train_lengths)}\")\n",
    "print(f\"  Average label length: {sum(train_lengths)/len(train_lengths):.2f}\")\n",
    "\n",
    "# Check character distribution\n",
    "all_chars = set(''.join(train_labels))\n",
    "print(f\"\\nUnique characters in dataset: {len(all_chars)}\")\n",
    "print(f\"Characters: {''.join(sorted(all_chars))}\")\n",
    "\n",
    "# Check if all characters are in our CHARS vocabulary\n",
    "unknown_chars = all_chars - set(CHARS)\n",
    "if unknown_chars:\n",
    "    print(f\"\\nWARNING: Unknown characters found: {unknown_chars}\")\n",
    "else:\n",
    "    print(f\"\\nAll characters are in the vocabulary!\")\n",
    "\n",
    "# Sample labels\n",
    "print(f\"\\nSample labels (first 10):\")\n",
    "for i in range(min(10, len(train_labels))):\n",
    "    print(f\"  {i+1}. {train_labels[i]} (length: {len(train_labels[i])})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T09:43:44.131541Z",
     "iopub.status.busy": "2025-11-08T09:43:44.130777Z",
     "iopub.status.idle": "2025-11-08T09:43:48.754637Z",
     "shell.execute_reply": "2025-11-08T09:43:48.753951Z",
     "shell.execute_reply.started": "2025-11-08T09:43:44.131514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"=== Comprehensive Tokenization Validation ===\\n\")\n",
    "\n",
    "# Test 1: Character Coverage\n",
    "print(\"1. Testing Character Coverage:\")\n",
    "print(f\"   CHARS vocabulary: {len(CHARS)} characters\")\n",
    "print(f\"   Token IDs: 1 to {len(CHARS)} (0 reserved for CTC blank)\")\n",
    "\n",
    "# Test each character individually\n",
    "all_pass = True\n",
    "failed_chars = []\n",
    "for char in CHARS:\n",
    "    encoded = encode_label(char)\n",
    "    decoded = decode_label(encoded)\n",
    "    if decoded != char:\n",
    "        all_pass = False\n",
    "        failed_chars.append(char)\n",
    "\n",
    "if all_pass:\n",
    "    print(f\"   All {len(CHARS)} characters encode/decode correctly!\")\n",
    "else:\n",
    "    print(f\"   Failed characters: {failed_chars}\")\n",
    "\n",
    "# Test 2: Token Range Validation\n",
    "print(\"\\n2. Testing Token Ranges:\")\n",
    "sample_size = min(100, len(train_dataset))\n",
    "min_token = float('inf')\n",
    "max_token = 0\n",
    "\n",
    "for i in range(sample_size):\n",
    "    _, encoded, _ = train_dataset[i]\n",
    "    if encoded:  # if not empty\n",
    "        min_token = min(min_token, min(encoded))\n",
    "        max_token = max(max_token, max(encoded))\n",
    "\n",
    "print(f\"   Token range in dataset: {min_token} to {max_token}\")\n",
    "print(f\"   Expected range: 1 to {len(CHARS)}\")\n",
    "if min_token >= 1 and max_token <= len(CHARS):\n",
    "    print(f\"   All tokens are within valid range!\")\n",
    "else:\n",
    "    print(f\"   Some tokens are out of range!\")\n",
    "\n",
    "# Test 3: Reversibility on Entire Dataset\n",
    "print(\"\\n3. Testing Reversibility (sample):\")\n",
    "sample_size = min(100, len(train_dataset))\n",
    "mismatches = 0\n",
    "\n",
    "for i in range(sample_size):\n",
    "    _, encoded, original = train_dataset[i]\n",
    "    decoded = decode_label(encoded)\n",
    "    if decoded != original:\n",
    "        mismatches += 1\n",
    "        if mismatches <= 5:  # Show first 5 mismatches\n",
    "            print(f\"   Mismatch: '{original}' → {encoded} → '{decoded}'\")\n",
    "\n",
    "if mismatches == 0:\n",
    "    print(f\"   All {sample_size} samples encode/decode correctly!\")\n",
    "else:\n",
    "    print(f\"   Found {mismatches} mismatches out of {sample_size} samples\")\n",
    "\n",
    "# Test 4: Check for Index 0 (should not appear in encoded labels)\n",
    "print(\"\\n4. Checking for blank token (0) in labels:\")\n",
    "has_blank = False\n",
    "for i in range(min(100, len(train_dataset))):\n",
    "    _, encoded, _ = train_dataset[i]\n",
    "    if 0 in encoded:\n",
    "        has_blank = True\n",
    "        break\n",
    "\n",
    "if not has_blank:\n",
    "    print(f\"   No blank tokens (0) found in encoded labels (correct!)\")\n",
    "else:\n",
    "    print(f\"   WARNING: Blank token (0) found in labels!\")\n",
    "\n",
    "# Test 5: Token Statistics\n",
    "print(\"\\n5. Token Statistics:\")\n",
    "all_tokens = []\n",
    "for i in range(min(1000, len(train_dataset))):\n",
    "    _, encoded, _ = train_dataset[i]\n",
    "    all_tokens.extend(encoded)\n",
    "\n",
    "from collections import Counter\n",
    "token_counts = Counter(all_tokens)\n",
    "most_common = token_counts.most_common(5)\n",
    "least_common = token_counts.most_common()[-5:]\n",
    "\n",
    "print(f\"   Total tokens analyzed: {len(all_tokens)}\")\n",
    "print(f\"   Unique tokens used: {len(token_counts)}\")\n",
    "print(f\"   Most common tokens:\")\n",
    "for token, count in most_common:\n",
    "    char = idx_to_char.get(token, '?')\n",
    "    print(f\"      Token {token} ('{char}'): {count} times\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TOKENIZATION VALIDATION COMPLETE\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T09:43:54.602735Z",
     "iopub.status.busy": "2025-11-08T09:43:54.602461Z",
     "iopub.status.idle": "2025-11-08T09:43:55.512219Z",
     "shell.execute_reply": "2025-11-08T09:43:55.511497Z",
     "shell.execute_reply.started": "2025-11-08T09:43:54.602716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CTC-CRNN Model for CAPTCHA Recognition\n",
    "\n",
    "Architecture based on:\n",
    "- Shi et al. \"An End-to-End Trainable Neural Network for Image-based Sequence Recognition\"\n",
    "- Best practices from CAPTCHA recognition research\n",
    "- WandB CRNN-CTC guide and Kaggle implementations\n",
    "\n",
    "Components:\n",
    "1. CNN (VGG-style): Feature extraction from images\n",
    "2. Bidirectional LSTM: Sequence modeling\n",
    "3. Fully Connected: Character classification\n",
    "4. CTC Loss: Alignment-free training\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# ResNet Block for Better Feature Learning\n",
    "# =====================================\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with skip connection\n",
    "    Helps with gradient flow and fine-grained feature discrimination\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Apply skip connection\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Recurrent Neural Network with ResNet-style CNN\n",
    "    \n",
    "    Architecture Flow:\n",
    "        Input Image (1, 80, 280)\n",
    "            ↓\n",
    "        ResNet CNN (6 residual blocks with skip connections)\n",
    "            ↓\n",
    "        Feature Maps (512, 5, 70)\n",
    "            ↓\n",
    "        Map to Sequence (70 time steps, 2560 features each)\n",
    "            ↓\n",
    "        Bidirectional LSTM (2 layers, 384 hidden)\n",
    "            ↓\n",
    "        Fully Connected Layer\n",
    "            ↓\n",
    "        Log Softmax\n",
    "            ↓\n",
    "        Output (70, N, 63) for CTC Loss\n",
    "    \n",
    "    Key improvement: ResNet skip connections for better gradient flow\n",
    "    and fine-grained feature discrimination.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        img_height=80,\n",
    "        img_width=280,\n",
    "        num_classes=63,  # 62 alphanumeric + 1 blank\n",
    "        hidden_size=256,\n",
    "        num_lstm_layers=2,\n",
    "        dropout=0.4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize CRNN model\n",
    "        \n",
    "        Args:\n",
    "            img_height: Input image height (default: 80)\n",
    "            img_width: Input image width (default: 280)\n",
    "            num_classes: Number of output classes including blank (default: 63)\n",
    "            hidden_size: LSTM hidden size (default: 256)\n",
    "            num_lstm_layers: Number of LSTM layers (default: 2)\n",
    "            dropout: Dropout rate for LSTM (default: 0.3)\n",
    "        \"\"\"\n",
    "        super(CRNN, self).__init__()\n",
    "        \n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # =====================================\n",
    "        # ResNet-style CNN Feature Extractor\n",
    "        # =====================================\n",
    "        # Skip connections improve gradient flow and feature discrimination\n",
    "        \n",
    "        # Initial conv: (1, 80, 280) → (64, 80, 280)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Pool1: (64, 80, 280) → (64, 40, 140)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # ResBlock layer1: (64, 40, 140) → (128, 40, 140)\n",
    "        self.layer1 = self._make_layer(64, 128, blocks=2)\n",
    "        \n",
    "        # Pool2: (128, 40, 140) → (128, 20, 70)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # ResBlock layer2: (128, 20, 70) → (256, 20, 70)\n",
    "        self.layer2 = self._make_layer(128, 256, blocks=2)\n",
    "        \n",
    "        # Pool3: (256, 20, 70) → (256, 10, 70)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 1))  # Only height\n",
    "        \n",
    "        # ResBlock layer3: (256, 10, 70) → (512, 10, 70)\n",
    "        self.layer3 = self._make_layer(256, 512, blocks=2)\n",
    "        \n",
    "        # Pool4: (512, 10, 70) → (512, 5, 70)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=(2, 1))  # Only height\n",
    "        \n",
    "        # Optional dropout\n",
    "        self.dropout = nn.Dropout2d(0.2)\n",
    "        \n",
    "        # Calculate RNN input size\n",
    "        # After all conv layers: (512 channels, 5 height, 70 width)\n",
    "        # We'll treat width (70) as sequence length\n",
    "        # Each time step will have 512 * 5 = 2560 features\n",
    "        self.map_to_seq_height = 5\n",
    "        self.map_to_seq_channels = 512\n",
    "        self.rnn_input_size = self.map_to_seq_height * self.map_to_seq_channels\n",
    "        \n",
    "        # =====================================\n",
    "        # Recurrent Layers (Bidirectional LSTM)\n",
    "        # =====================================\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.rnn_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            bidirectional=True,\n",
    "            dropout=0.3 if num_lstm_layers > 1 else 0,\n",
    "            batch_first=False  # (T, N, C) format for CTC\n",
    "        )\n",
    "        \n",
    "        # =====================================\n",
    "        # Fully Connected Layer\n",
    "        # =====================================\n",
    "        # Map from LSTM output (hidden_size * 2 due to bidirectional) to num_classes\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _make_layer(self, in_channels, out_channels, blocks):\n",
    "        \"\"\"\n",
    "        Create a layer with multiple residual blocks\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Input channels\n",
    "            out_channels: Output channels\n",
    "            blocks: Number of residual blocks\n",
    "        \n",
    "        Returns:\n",
    "            nn.Sequential of residual blocks\n",
    "        \"\"\"\n",
    "        downsample = None\n",
    "        if in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride=1, downsample=downsample))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            x: Input images of shape (N, 1, H, W)\n",
    "               N = batch size\n",
    "               H = height (80)\n",
    "               W = width (280)\n",
    "        \n",
    "        Returns:\n",
    "            log_probs: Log probabilities of shape (T, N, C)\n",
    "                      T = sequence length (70)\n",
    "                      N = batch size\n",
    "                      C = num_classes (63)\n",
    "        \"\"\"\n",
    "        # ===== 1. ResNet CNN Feature Extraction =====\n",
    "        x = self.conv1(x)      # (N, 64, 80, 280)\n",
    "        x = self.pool1(x)      # (N, 64, 40, 140)\n",
    "        \n",
    "        x = self.layer1(x)     # (N, 128, 40, 140) - ResBlock\n",
    "        x = self.pool2(x)      # (N, 128, 20, 70)\n",
    "        \n",
    "        x = self.layer2(x)     # (N, 256, 20, 70) - ResBlock\n",
    "        x = self.pool3(x)      # (N, 256, 10, 70)\n",
    "        \n",
    "        x = self.layer3(x)     # (N, 512, 10, 70) - ResBlock\n",
    "        x = self.pool4(x)      # (N, 512, 5, 70)\n",
    "        \n",
    "        conv_out = self.dropout(x)  # (N, 512, 5, 70)\n",
    "        \n",
    "        batch_size, channels, height, width = conv_out.size()\n",
    "        \n",
    "        # ===== 2. Map to Sequence =====\n",
    "        # Reshape CNN output to sequence format\n",
    "        # (N, C, H, W) → (N, W, C*H)\n",
    "        # Width becomes sequence length, C*H becomes features per time step\n",
    "        conv_out = conv_out.permute(0, 3, 1, 2)  # (N, 70, 512, 5)\n",
    "        conv_out = conv_out.reshape(batch_size, width, channels * height)  # (N, 70, 2560)\n",
    "        \n",
    "        # ===== 3. Prepare for LSTM =====\n",
    "        # LSTM expects (T, N, features) when batch_first=False\n",
    "        rnn_input = conv_out.permute(1, 0, 2)  # (70, N, 2560)\n",
    "        \n",
    "        # ===== 4. Bidirectional LSTM =====\n",
    "        rnn_output, _ = self.rnn(rnn_input)  # (70, N, 512)\n",
    "        # Output size: hidden_size * 2 (bidirectional) = 256 * 2 = 512\n",
    "        \n",
    "        # ===== 5. Fully Connected Layer =====\n",
    "        T, N, hidden = rnn_output.size()\n",
    "        \n",
    "        # Reshape to apply FC layer\n",
    "        rnn_output = rnn_output.reshape(T * N, hidden)  # (70*N, 512)\n",
    "        output = self.fc(rnn_output)  # (70*N, 63)\n",
    "        output = output.reshape(T, N, self.num_classes)  # (70, N, 63)\n",
    "        \n",
    "        # ===== 6. Log Softmax for CTC Loss =====\n",
    "        log_probs = F.log_softmax(output, dim=2)  # (70, N, 63)\n",
    "        \n",
    "        return log_probs\n",
    "    \n",
    "    def get_sequence_length(self):\n",
    "        \"\"\"\n",
    "        Get the output sequence length after CNN layers\n",
    "        \n",
    "        Returns:\n",
    "            Sequence length (70 for input width 280)\n",
    "        \"\"\"\n",
    "        # Width reduction through pooling:\n",
    "        # Original: 280\n",
    "        # After MaxPool 2x2 (stride 2): 140\n",
    "        # After MaxPool 2x2 (stride 2): 70\n",
    "        # After MaxPool (2,1): 70 (no width change)\n",
    "        # After MaxPool (2,1): 70 (no width change)\n",
    "        return 70\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count total and trainable parameters\"\"\"\n",
    "        total = sum(p.numel() for p in self.parameters())\n",
    "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return total, trainable\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# Helper Functions\n",
    "# =====================================\n",
    "\n",
    "def create_model(num_classes=63, hidden_size=256, device='cpu'):\n",
    "    \"\"\"\n",
    "    Create and initialize CRNN model\n",
    "    \n",
    "    Args:\n",
    "        num_classes: Number of output classes (default: 63)\n",
    "        hidden_size: LSTM hidden size (default: 256)\n",
    "        device: Device to put model on (default: 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        model: Initialized CRNN model\n",
    "    \"\"\"\n",
    "    model = CRNN(\n",
    "        img_height=80,\n",
    "        img_width=280,\n",
    "        num_classes=num_classes,\n",
    "        hidden_size=hidden_size,\n",
    "        num_lstm_layers=2,\n",
    "        dropout=0.5\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Print model summary\n",
    "    total_params, trainable_params = model.count_parameters()\n",
    "    print(f\"Model created successfully!\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"Test model with dummy input\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Testing CRNN Model\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create model\n",
    "    model = CRNN(img_height=80, img_width=280, num_classes=63)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dummy input\n",
    "    batch_size = 4\n",
    "    dummy_input = torch.randn(batch_size, 1, 80, 280)\n",
    "    \n",
    "    print(f\"\\nInput shape: {dummy_input.shape}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(dummy_input)\n",
    "    \n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Expected: (70, {batch_size}, 63)\")\n",
    "    \n",
    "    # Check output dimensions\n",
    "    T, N, C = output.shape\n",
    "    assert T == 70, f\"Sequence length should be 70, got {T}\"\n",
    "    assert N == batch_size, f\"Batch size should be {batch_size}, got {N}\"\n",
    "    assert C == 63, f\"Num classes should be 63, got {C}\"\n",
    "    \n",
    "    print(\"\\nModel test passed!\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total, trainable = model.count_parameters()\n",
    "    print(f\"\\nModel Statistics:\")\n",
    "    print(f\"  Total parameters: {total:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable:,}\")\n",
    "    print(f\"  Model size: ~{total * 4 / 1024 / 1024:.2f} MB (float32)\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T09:44:07.410712Z",
     "iopub.status.busy": "2025-11-08T09:44:07.410083Z",
     "iopub.status.idle": "2025-11-08T09:44:07.437088Z",
     "shell.execute_reply": "2025-11-08T09:44:07.436379Z",
     "shell.execute_reply.started": "2025-11-08T09:44:07.410688Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training Utilities for CTC-CRNN CAPTCHA Recognition\n",
    "\n",
    "Includes:\n",
    "- Training loop\n",
    "- Validation loop\n",
    "- CTC greedy decoder\n",
    "- Learning rate scheduling\n",
    "- Checkpoint management\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# CTC Loss\n",
    "# =====================================\n",
    "\n",
    "def get_ctc_loss(blank=0, reduction='mean', zero_infinity=True):\n",
    "    \"\"\"\n",
    "    Create CTC loss function\n",
    "    \n",
    "    Args:\n",
    "        blank: Index of blank token (default: 0)\n",
    "        reduction: Loss reduction method (default: 'mean')\n",
    "        zero_infinity: Whether to zero infinite losses (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        CTC loss criterion\n",
    "    \"\"\"\n",
    "    return nn.CTCLoss(blank=blank, reduction=reduction, zero_infinity=zero_infinity)\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# CTC Decoders\n",
    "# =====================================\n",
    "\n",
    "def ctc_decode_greedy(log_probs, input_lengths, idx_to_char):\n",
    "    \"\"\"\n",
    "    Greedy CTC decoder - simple and fast\n",
    "    \n",
    "    Args:\n",
    "        log_probs: Log probabilities from model (T, N, C)\n",
    "        input_lengths: Actual sequence lengths (N,)\n",
    "        idx_to_char: Dictionary mapping token IDs to characters\n",
    "    \n",
    "    Returns:\n",
    "        List of decoded strings\n",
    "    \"\"\"\n",
    "    # Get most probable class at each time step\n",
    "    _, max_indices = torch.max(log_probs, dim=2)  # (T, N)\n",
    "    max_indices = max_indices.transpose(0, 1)  # (N, T)\n",
    "    \n",
    "    decoded_texts = []\n",
    "    for i, length in enumerate(input_lengths):\n",
    "        # Get predictions for this sequence\n",
    "        pred_tokens = max_indices[i, :length].tolist()\n",
    "        \n",
    "        # CTC collapse: remove consecutive duplicates and blanks\n",
    "        collapsed = []\n",
    "        prev_token = None\n",
    "        for token in pred_tokens:\n",
    "            if token != 0 and token != prev_token:  # 0 is blank\n",
    "                collapsed.append(token)\n",
    "            prev_token = token\n",
    "        \n",
    "        # Decode to text\n",
    "        text = ''.join([idx_to_char.get(t, '') for t in collapsed])\n",
    "        decoded_texts.append(text)\n",
    "    \n",
    "    return decoded_texts\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# Training Functions\n",
    "# =====================================\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, idx_to_char=None):\n",
    "    \"\"\"\n",
    "    Train model for one epoch\n",
    "    \n",
    "    Args:\n",
    "        model: CRNN model\n",
    "        dataloader: Training data loader\n",
    "        criterion: CTC loss function\n",
    "        optimizer: Optimizer\n",
    "        device: Device (cuda/cpu)\n",
    "        idx_to_char: Character mapping for decoding (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Average loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    with tqdm(dataloader, desc=\"Training\", unit=\"batch\") as pbar:\n",
    "        for batch_idx, (images, targets, input_lengths, target_lengths, _) in enumerate(pbar):\n",
    "            # Move to device\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            input_lengths = input_lengths.to(device)\n",
    "            target_lengths = target_lengths.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            log_probs = model(images)  # (T, N, C)\n",
    "            \n",
    "            # Calculate CTC loss\n",
    "            loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
    "            \n",
    "            # Check for invalid loss\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"\\nWarning: Invalid loss at batch {batch_idx}: {loss.item()}\")\n",
    "                continue\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (important for LSTM)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update progress bar\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device, idx_to_char):\n",
    "    \"\"\"\n",
    "    Validate model using greedy decoding\n",
    "    \n",
    "    Args:\n",
    "        model: CRNN model\n",
    "        dataloader: Validation data loader\n",
    "        criterion: CTC loss function\n",
    "        device: Device (cuda/cpu)\n",
    "        idx_to_char: Character mapping for decoding\n",
    "    \n",
    "    Returns:\n",
    "        avg_loss: Average validation loss\n",
    "        accuracy: Character-level accuracy\n",
    "        word_accuracy: Word-level accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_chars = 0\n",
    "    total_chars = 0\n",
    "    correct_words = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(dataloader, desc=\"Validation\", unit=\"batch\") as pbar:\n",
    "            for images, targets, input_lengths, target_lengths, label_texts in pbar:\n",
    "                # Move to device\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "                input_lengths = input_lengths.to(device)\n",
    "                target_lengths = target_lengths.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                log_probs = model(images)  # (T, N, C)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
    "                \n",
    "                if not (torch.isnan(loss) or torch.isinf(loss)):\n",
    "                    total_loss += loss.item()\n",
    "                \n",
    "                # Decode predictions using greedy decoder\n",
    "                predictions = ctc_decode_greedy(log_probs, input_lengths, idx_to_char)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                for pred, gt in zip(predictions, label_texts):\n",
    "                    # Word accuracy\n",
    "                    if pred == gt:\n",
    "                        correct_words += 1\n",
    "                    total_words += 1\n",
    "                    \n",
    "                    # Character accuracy\n",
    "                    for p, g in zip(pred, gt):\n",
    "                        if p == g:\n",
    "                            correct_chars += 1\n",
    "                    total_chars += len(gt)\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'acc': f'{correct_words/total_words:.4f}'\n",
    "                })\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    char_accuracy = correct_chars / total_chars if total_chars > 0 else 0\n",
    "    word_accuracy = correct_words / total_words if total_words > 0 else 0\n",
    "    \n",
    "    return avg_loss, char_accuracy, word_accuracy\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# Checkpoint Management\n",
    "# =====================================\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, accuracy, filepath):\n",
    "    \"\"\"\n",
    "    Save model checkpoint\n",
    "    \n",
    "    Args:\n",
    "        model: Model to save\n",
    "        optimizer: Optimizer state\n",
    "        epoch: Current epoch\n",
    "        loss: Current loss\n",
    "        accuracy: Current accuracy\n",
    "        filepath: Path to save checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Checkpoint saved: {filepath}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, filepath, device):\n",
    "    \"\"\"\n",
    "    Load model checkpoint\n",
    "    \n",
    "    Args:\n",
    "        model: Model to load weights into\n",
    "        optimizer: Optimizer to load state into\n",
    "        filepath: Path to checkpoint\n",
    "        device: Device to load on\n",
    "    \n",
    "    Returns:\n",
    "        epoch: Last epoch number\n",
    "        loss: Last loss value\n",
    "        accuracy: Last accuracy value\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    accuracy = checkpoint.get('accuracy', 0.0)\n",
    "    \n",
    "    print(f\"Checkpoint loaded: {filepath}\")\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return epoch, loss, accuracy\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# Full Training Loop\n",
    "# =====================================\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    idx_to_char,\n",
    "    num_epochs=50,\n",
    "    learning_rate=0.001,\n",
    "    device='cuda',\n",
    "    checkpoint_dir='checkpoints',\n",
    "    save_every=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete training loop\n",
    "    \n",
    "    Args:\n",
    "        model: CRNN model\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        idx_to_char: Character mapping\n",
    "        num_epochs: Number of epochs to train\n",
    "        learning_rate: Initial learning rate\n",
    "        device: Device (cuda/cpu)\n",
    "        checkpoint_dir: Directory to save checkpoints\n",
    "        save_every: Save checkpoint every N epochs\n",
    "    \n",
    "    Returns:\n",
    "        Training history dictionary\n",
    "    \"\"\"\n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Setup\n",
    "    model = model.to(device)\n",
    "    criterion = get_ctc_loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_char_acc': [],\n",
    "        'val_word_acc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    \n",
    "    print(f\"\\nStarting training for {num_epochs} epochs\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, idx_to_char)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, char_acc, word_acc = validate(model, val_loader, criterion, device, idx_to_char)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_char_acc'].append(char_acc)\n",
    "        history['val_word_acc'].append(word_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Char Accuracy: {char_acc:.4f}\")\n",
    "        print(f\"  Word Accuracy: {word_acc:.4f}\")\n",
    "        print(f\"  Learning Rate: {current_lr}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "            save_checkpoint(model, optimizer, epoch+1, val_loss, word_acc, checkpoint_path)\n",
    "        \n",
    "        # Save best model\n",
    "        if word_acc > best_acc:\n",
    "            best_acc = word_acc\n",
    "            best_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "            save_checkpoint(model, optimizer, epoch+1, val_loss, word_acc, best_path)\n",
    "            print(f\"  New best model! Accuracy: {best_acc:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training completed!\")\n",
    "    print(f\"Best word accuracy: {best_acc:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Training utilities loaded successfully!\")\n",
    "    print(\"\\nAvailable functions:\")\n",
    "    print(\"  - train_one_epoch()\")\n",
    "    print(\"  - validate()\")\n",
    "    print(\"  - ctc_decode_greedy()\")\n",
    "    print(\"  - save_checkpoint()\")\n",
    "    print(\"  - load_checkpoint()\")\n",
    "    print(\"  - train_model() [Full training loop]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T09:44:36.248797Z",
     "iopub.status.busy": "2025-11-08T09:44:36.248519Z",
     "iopub.status.idle": "2025-11-08T11:39:51.619691Z",
     "shell.execute_reply": "2025-11-08T11:39:51.617825Z",
     "shell.execute_reply.started": "2025-11-08T09:44:36.248777Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training Configuration and Setup\n",
    "\n",
    "# Set device (auto-detects GPU/MPS/CPU)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using device: mps (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using device: cuda ({torch.cuda.get_device_name(0)})\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using device: cpu\")\n",
    "\n",
    "# Dataset paths\n",
    "TRAIN_DIR = '/kaggle/input/captcha-training-images/captacha_dataset/train'\n",
    "TEST_DIR = '/kaggle/input/captcha-training-images/captacha_dataset/test'\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 150\n",
    "LEARNING_RATE = 0.001\n",
    "HIDDEN_SIZE = 384\n",
    "NUM_LSTM_LAYERS = 2\n",
    "NUM_WORKERS = 2\n",
    "SAVE_EVERY = 5  # Save checkpoint every N epochs\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# =====================================\n",
    "# Create Datasets & DataLoaders\n",
    "# =====================================\n",
    "\n",
    "print(\"\\nLoading datasets...\")\n",
    "\n",
    "# Create datasets with augmentation for training\n",
    "train_dataset = CaptchaCTCDataset(TRAIN_DIR, augment=True)\n",
    "test_dataset = CaptchaCTCDataset(TEST_DIR, augment=False)\n",
    "\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=ctc_collate_fn,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=ctc_collate_fn,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# =====================================\n",
    "# Create Model\n",
    "# =====================================\n",
    "\n",
    "print(\"\\nCreating model...\")\n",
    "model = CRNN(\n",
    "    img_height=80,\n",
    "    img_width=280,\n",
    "    num_classes=63,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_lstm_layers=NUM_LSTM_LAYERS,\n",
    "    dropout=0.3\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model created!\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# =====================================\n",
    "# Training Setup\n",
    "# =====================================\n",
    "\n",
    "# CTC Loss with Label Smoothing\n",
    "class CTCLossWithLabelSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    CTC Loss with label smoothing regularization.\n",
    "    Prevents overconfident predictions and improves generalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, blank=0, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.ctc_loss = nn.CTCLoss(blank=blank, reduction='mean', zero_infinity=True)\n",
    "        self.smoothing = smoothing\n",
    "        self.blank = blank\n",
    "    \n",
    "    def forward(self, log_probs, targets, input_lengths, target_lengths):\n",
    "        # Compute standard CTC loss\n",
    "        loss = self.ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
    "        \n",
    "        # Apply label smoothing if configured\n",
    "        if self.smoothing > 0:\n",
    "            smoothed_loss = -(log_probs.mean())\n",
    "            loss = (1 - self.smoothing) * loss + self.smoothing * smoothed_loss\n",
    "        \n",
    "        return loss\n",
    "\n",
    "criterion = CTCLossWithLabelSmoothing(blank=0, smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "# Learning rate warmup configuration\n",
    "WARMUP_EPOCHS = 5\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer, \n",
    "    start_factor=0.1,  # Start at 10% of base learning rate\n",
    "    total_iters=WARMUP_EPOCHS\n",
    ")\n",
    "\n",
    "# Initialize training history tracking\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'char_acc': []\n",
    "}\n",
    "\n",
    "best_acc = 0.0\n",
    "\n",
    "# Early stopping configuration\n",
    "EARLY_STOP_PATIENCE = 10\n",
    "early_stop_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Starting Training - {NUM_EPOCHS} Epochs\")\n",
    "print(f\"  Early Stopping: Patience = {EARLY_STOP_PATIENCE} epochs\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# =====================================\n",
    "# Training Loop\n",
    "# =====================================\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # ===== Train =====\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with tqdm(train_loader, desc=\"Training\", unit=\"batch\") as pbar:\n",
    "        for batch_idx, (images, targets, input_lengths, target_lengths, _) in enumerate(pbar):\n",
    "            # Move to device\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            input_lengths = input_lengths.to(device)\n",
    "            target_lengths = target_lengths.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            log_probs = model(images)\n",
    "            loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
    "            \n",
    "            # Skip if loss is invalid\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"\\nWarning: Invalid loss at batch {batch_idx}\")\n",
    "                continue\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # ===== Validate =====\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    correct_chars = 0\n",
    "    total_chars = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_loader, desc=\"Validation\", unit=\"batch\") as pbar:\n",
    "            for images, targets, input_lengths, target_lengths, label_texts in pbar:\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "                input_lengths = input_lengths.to(device)\n",
    "                target_lengths = target_lengths.to(device)\n",
    "                \n",
    "                log_probs = model(images)\n",
    "                loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
    "                \n",
    "                if not (torch.isnan(loss) or torch.isinf(loss)):\n",
    "                    val_loss += loss.item()\n",
    "                \n",
    "                # Decode predictions using greedy decoder\n",
    "                _, max_indices = torch.max(log_probs, dim=2)\n",
    "                max_indices = max_indices.transpose(0, 1)\n",
    "                \n",
    "                predictions = []\n",
    "                for i, length in enumerate(input_lengths):\n",
    "                    pred_tokens = max_indices[i, :length].tolist()\n",
    "                    collapsed = []\n",
    "                    prev = None\n",
    "                    for token in pred_tokens:\n",
    "                        if token != 0 and token != prev:\n",
    "                            collapsed.append(token)\n",
    "                        prev = token\n",
    "                    pred_text = ''.join([idx_to_char.get(t, '') for t in collapsed])\n",
    "                    predictions.append(pred_text)\n",
    "                \n",
    "                for pred, gt in zip(predictions, label_texts):\n",
    "                    # Compute sequence-level accuracy (entire string must match)\n",
    "                    if pred == gt:\n",
    "                        correct += 1\n",
    "                    total += 1\n",
    "                    \n",
    "                    # Compute character-level accuracy\n",
    "                    for p, g in zip(pred, gt):\n",
    "                        if p == g:\n",
    "                            correct_chars += 1\n",
    "                    total_chars += len(gt)\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'seq_acc': f'{correct/total:.4f}',\n",
    "                    'char_acc': f'{correct_chars/total_chars:.4f}' if total_chars > 0 else '0.0000'\n",
    "                })\n",
    "    \n",
    "    val_loss = val_loss / len(test_loader)\n",
    "    val_acc = correct / total\n",
    "    char_acc = correct_chars / total_chars if total_chars > 0 else 0\n",
    "    \n",
    "    # ===== Update Learning Rate =====\n",
    "    # Use warmup for first epochs, then switch to ReduceLROnPlateau\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        warmup_scheduler.step()\n",
    "    else:\n",
    "        scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # ===== Store History =====\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['char_acc'].append(char_acc)\n",
    "    \n",
    "    # ===== Early Stopping Check =====\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "    \n",
    "    # ===== Print Summary =====\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"   Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"   Val Loss:   {val_loss:.4f}\")\n",
    "    print(f\"   Val Acc (Seq): {val_acc:.4f}\")\n",
    "    print(f\"   Val Acc (Char): {char_acc:.4f}\")\n",
    "    print(f\"   LR:         {current_lr:.6f}\")\n",
    "    print(f\"   Early Stop: {early_stop_counter}/{EARLY_STOP_PATIENCE}\")\n",
    "    \n",
    "    # ===== Save Checkpoint =====\n",
    "    if (epoch + 1) % SAVE_EVERY == 0:\n",
    "        checkpoint_path = f'checkpoints/checkpoint_epoch_{epoch+1}.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'history': history\n",
    "        }, checkpoint_path)\n",
    "        print(f\"   Checkpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    # ===== Save Best Model =====\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc\n",
    "        }, 'checkpoints/best_model.pth')\n",
    "        print(f\"   New best model! Accuracy: {best_acc:.4f}\")\n",
    "    \n",
    "    # ===== Early Stopping Trigger =====\n",
    "    if early_stop_counter >= EARLY_STOP_PATIENCE:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Early stopping triggered! No improvement for {EARLY_STOP_PATIENCE} epochs.\")\n",
    "        print(f\"{'='*70}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Best Accuracy (Seq): {best_acc:.4f}\")\n",
    "print(f\"Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Final Val Acc (Seq): {history['val_acc'][-1]:.4f}\")\n",
    "print(f\"Final Val Acc (Char): {history['char_acc'][-1]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T12:38:34.903866Z",
     "iopub.status.busy": "2025-11-08T12:38:34.903607Z",
     "iopub.status.idle": "2025-11-08T12:38:35.274258Z",
     "shell.execute_reply": "2025-11-08T12:38:35.273169Z",
     "shell.execute_reply.started": "2025-11-08T12:38:34.903847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history['val_acc'], label='Validation Accuracy', color='green', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"   Best Accuracy: {max(history['val_acc']):.4f}\")\n",
    "print(f\"   Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"   Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"   Final Val Acc: {history['val_acc'][-1]:.4f}\")\n",
    "print(f\"\\nPlot saved as 'training_history.png'\")\n",
    "\n",
    "# Package checkpoints for download (Kaggle/Colab)\n",
    "try:\n",
    "    import subprocess\n",
    "    subprocess.run(['zip', '-r', 'captcha_checkpoints.zip', 'checkpoints/'], check=True)\n",
    "    print(\"Checkpoints packaged as 'captcha_checkpoints.zip'\")\n",
    "    print(\"  Download from Output tab (Kaggle) or Files panel (Colab)\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Manual zip - run: !zip -r captcha_checkpoints.zip checkpoints/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T12:39:02.716348Z",
     "iopub.status.busy": "2025-11-08T12:39:02.715630Z",
     "iopub.status.idle": "2025-11-08T12:39:02.740500Z",
     "shell.execute_reply": "2025-11-08T12:39:02.739661Z",
     "shell.execute_reply.started": "2025-11-08T12:39:02.716323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Analyze prediction errors to understand what's failing\n",
    "# Run this AFTER training completes to get insights for next steps\n",
    "\n",
    "def analyze_errors(model, dataloader, device, idx_to_char, num_samples=200):\n",
    "    \"\"\"\n",
    "    Analyze common prediction errors to identify patterns\n",
    "    \n",
    "    Args:\n",
    "        model: Trained CRNN model\n",
    "        dataloader: Test data loader\n",
    "        device: Device (cuda/cpu)\n",
    "        idx_to_char: Character mapping\n",
    "        num_samples: Number of error samples to collect\n",
    "    \n",
    "    Returns:\n",
    "        errors: List of error dictionaries\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    errors = []\n",
    "    correct_samples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets, input_lengths, target_lengths, label_texts in dataloader:\n",
    "            if len(errors) >= num_samples:\n",
    "                break\n",
    "                \n",
    "            images = images.to(device)\n",
    "            log_probs = model(images)\n",
    "            \n",
    "            # Decode (greedy)\n",
    "            _, max_indices = torch.max(log_probs, dim=2)\n",
    "            max_indices = max_indices.transpose(0, 1)\n",
    "            \n",
    "            for i in range(len(label_texts)):\n",
    "                pred_tokens = max_indices[i, :70].tolist()\n",
    "                collapsed = []\n",
    "                prev = None\n",
    "                for token in pred_tokens:\n",
    "                    if token != 0 and token != prev:\n",
    "                        collapsed.append(token)\n",
    "                    prev = token\n",
    "                \n",
    "                pred_text = ''.join([idx_to_char.get(t, '') for t in collapsed])\n",
    "                gt_text = label_texts[i]\n",
    "                \n",
    "                if pred_text != gt_text:\n",
    "                    # Calculate edit distance\n",
    "                    import difflib\n",
    "                    errors.append({\n",
    "                        'predicted': pred_text,\n",
    "                        'ground_truth': gt_text,\n",
    "                        'len_diff': len(pred_text) - len(gt_text),\n",
    "                        'len_pred': len(pred_text),\n",
    "                        'len_gt': len(gt_text)\n",
    "                    })\n",
    "                else:\n",
    "                    correct_samples.append(gt_text)\n",
    "    \n",
    "    # Analyze error patterns\n",
    "    from collections import Counter\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Error Analysis - {len(errors)} errors collected\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # 1. Length Analysis\n",
    "    len_diffs = [e['len_diff'] for e in errors]\n",
    "    print(f\"\\nLength Errors:\")\n",
    "    too_short = sum(1 for d in len_diffs if d < 0)\n",
    "    too_long = sum(1 for d in len_diffs if d > 0)\n",
    "    correct_len = sum(1 for d in len_diffs if d == 0)\n",
    "    \n",
    "    print(f\"  Too short (missing chars):   {too_short:3d} ({too_short/len(errors)*100:5.1f}%)\")\n",
    "    print(f\"  Too long (extra chars):      {too_long:3d} ({too_long/len(errors)*100:5.1f}%)\")\n",
    "    print(f\"  Correct length (wrong char): {correct_len:3d} ({correct_len/len(errors)*100:5.1f}%)\")\n",
    "    \n",
    "    # 2. Character Confusion Matrix\n",
    "    char_errors = []\n",
    "    positional_errors = {'start': 0, 'middle': 0, 'end': 0}\n",
    "    \n",
    "    for e in errors:\n",
    "        pred, gt = e['predicted'], e['ground_truth']\n",
    "        min_len = min(len(pred), len(gt))\n",
    "        \n",
    "        for i in range(min_len):\n",
    "            if pred[i] != gt[i]:\n",
    "                char_errors.append((gt[i], pred[i]))\n",
    "                \n",
    "                # Track position\n",
    "                if i < 2:\n",
    "                    positional_errors['start'] += 1\n",
    "                elif i >= len(gt) - 2:\n",
    "                    positional_errors['end'] += 1\n",
    "                else:\n",
    "                    positional_errors['middle'] += 1\n",
    "    \n",
    "    print(f\"\\nTop 15 Character Confusions:\")\n",
    "    confusion_counts = Counter(char_errors)\n",
    "    for idx, ((gt_char, pred_char), count) in enumerate(confusion_counts.most_common(15), 1):\n",
    "        print(f\"  {idx:2d}. '{gt_char}' → '{pred_char}': {count:3d} times\")\n",
    "    \n",
    "    # 3. Positional error analysis\n",
    "    total_pos_errors = sum(positional_errors.values())\n",
    "    if total_pos_errors > 0:\n",
    "        print(f\"\\nError Position Distribution:\")\n",
    "        print(f\"  Start (0-1):   {positional_errors['start']:3d} ({positional_errors['start']/total_pos_errors*100:5.1f}%)\")\n",
    "        print(f\"  Middle (2-n):  {positional_errors['middle']:3d} ({positional_errors['middle']/total_pos_errors*100:5.1f}%)\")\n",
    "        print(f\"  End (last 2):  {positional_errors['end']:3d} ({positional_errors['end']/total_pos_errors*100:5.1f}%)\")\n",
    "    \n",
    "    # 4. Length distribution\n",
    "    len_gt_dist = Counter([e['len_gt'] for e in errors])\n",
    "    print(f\"\\nError Distribution by Ground Truth Length:\")\n",
    "    for length in sorted(len_gt_dist.keys()):\n",
    "        count = len_gt_dist[length]\n",
    "        print(f\"  Length {length}: {count:3d} errors ({count/len(errors)*100:5.1f}%)\")\n",
    "    \n",
    "    # 5. Sample errors (most informative)\n",
    "    print(f\"\\nSample Errors (showing first 20):\")\n",
    "    print(f\"{'':4s}{'Ground Truth':<20s} → {'Predicted':<20s} {'Length'}\")\n",
    "    print(f\"    {'-'*20}   {'-'*20} {'-'*8}\")\n",
    "    \n",
    "    for i, e in enumerate(errors[:20], 1):\n",
    "        gt_display = e['ground_truth'][:18] if len(e['ground_truth']) > 18 else e['ground_truth']\n",
    "        pred_display = e['predicted'][:18] if len(e['predicted']) > 18 else e['predicted']\n",
    "        \n",
    "        len_indicator = \"\"\n",
    "        if e['len_diff'] < 0:\n",
    "            len_indicator = f\"(-{abs(e['len_diff'])})\"\n",
    "        elif e['len_diff'] > 0:\n",
    "            len_indicator = f\"(+{e['len_diff']})\"\n",
    "        else:\n",
    "            len_indicator = \"(same)\"\n",
    "        \n",
    "        print(f\"  {i:2d}. {gt_display:<20s} → {pred_display:<20s} {len_indicator}\")\n",
    "    \n",
    "    # 6. Key Insights\n",
    "    print(f\"\\nKey Insights:\")\n",
    "    \n",
    "    if correct_len / len(errors) > 0.5:\n",
    "        print(f\"  - 50%+ errors have correct length → Focus on character recognition\")\n",
    "    else:\n",
    "        print(f\"  - <50% errors have correct length → Sequence modeling issue\")\n",
    "    \n",
    "    if too_short > too_long * 1.5:\n",
    "        print(f\"  - Model tends to predict too short → CTC blanks collapsing too much\")\n",
    "    elif too_long > too_short * 1.5:\n",
    "        print(f\"  - Model tends to predict too long → False positives in character detection\")\n",
    "    \n",
    "    # Check for common confusions\n",
    "    if confusion_counts:\n",
    "        top_confusion = confusion_counts.most_common(1)[0]\n",
    "        if top_confusion[1] > len(errors) * 0.1:  # More than 10% of errors\n",
    "            print(f\"  - High confusion: '{top_confusion[0][0]}' ↔ '{top_confusion[0][1]}' ({top_confusion[1]} times)\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    \n",
    "    return errors\n",
    "\n",
    "\n",
    "# Run error analysis on test set\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Running Error Analysis...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "errors = analyze_errors(model, test_loader, device, idx_to_char, num_samples=200)\n",
    "\n",
    "print(\"\\nError analysis complete!\")\n",
    "print(f\"  Next steps will depend on the patterns observed above.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8590641,
     "sourceId": 13529268,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8546501,
     "sourceId": 13655550,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
